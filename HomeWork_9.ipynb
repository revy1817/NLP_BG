{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HomeWork.ipynb",
      "provenance": [],
      "mount_file_id": "1a1jiEI5-s2xhXFqfVlMSt4MjmEJB_X5u",
      "authorship_tag": "ABX9TyOxX54nP+c26GTweICWIO0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revy1817/NLP_BG/blob/main/HomeWork_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8YKtDJvHRM7M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_folder = '/content/drive/MyDrive/Colab Notebooks/Учеба/GeekBrains/Введение в обработку естественного языка/Less 9/TrumpSpeech/'"
      ],
      "metadata": {
        "id": "7CBSsq3eSwCb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмем датасет выступлений трампа, и сдаем нейросеть которая будет генерировать его политические спичи"
      ],
      "metadata": {
        "id": "9Z2iayYgZfUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "files_path = os.path.join(path_folder,'*.txt')\n",
        "\n",
        "for path in glob(files_path):\n",
        "  text_local = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "  text += ' ' + text_local\n",
        "  # length of text is the number of characters in it\n",
        "  print('Length of one file text: {} characters'.format(len(text_local)))\n",
        "\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK0D7WpETZ-J",
        "outputId": "a038d400-4399-4e99-da81-39f5e8da54b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of one file text: 49737 characters\n",
            "Length of one file text: 58149 characters\n",
            "Length of one file text: 60811 characters\n",
            "Length of one file text: 63409 characters\n",
            "Length of one file text: 64514 characters\n",
            "Length of one file text: 38989 characters\n",
            "Length of one file text: 76668 characters\n",
            "Length of one file text: 57081 characters\n",
            "Length of one file text: 50472 characters\n",
            "Length of one file text: 52640 characters\n",
            "Length of one file text: 87029 characters\n",
            "Length of one file text: 63291 characters\n",
            "Length of one file text: 36830 characters\n",
            "Length of one file text: 61155 characters\n",
            "Length of one file text: 58316 characters\n",
            "Length of one file text: 74654 characters\n",
            "Length of one file text: 67534 characters\n",
            "Length of one file text: 49376 characters\n",
            "Length of one file text: 49082 characters\n",
            "Length of one file text: 78082 characters\n",
            "Length of one file text: 63122 characters\n",
            "Length of one file text: 89735 characters\n",
            "Length of one file text: 37173 characters\n",
            "Length of one file text: 14610 characters\n",
            "Length of one file text: 34470 characters\n",
            "Length of one file text: 58727 characters\n",
            "Length of one file text: 54931 characters\n",
            "Length of one file text: 55388 characters\n",
            "Length of one file text: 52215 characters\n",
            "Length of one file text: 54664 characters\n",
            "Length of one file text: 51794 characters\n",
            "Length of one file text: 51128 characters\n",
            "Length of one file text: 44934 characters\n",
            "Length of one file text: 65020 characters\n",
            "Length of one file text: 95760 characters\n",
            "Length of text: 2021525 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJRKxu3ZXMwQ",
        "outputId": "a16f367f-727c-407d-f9da-7f0673693ec9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "s4lkpWaWXTHx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int, text, len(text_as_int), len(text)"
      ],
      "metadata": {
        "id": "i2My9sXeXUCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "jn5MzS7iYPqR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "QGEH5DEaYfFN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ewFh9cYx4H",
        "outputId": "44543ada-af54-4186-8b18-b8359e05d344"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "SEw_3bGvY0yO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "                                 \n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "         tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        \n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "                                   \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "4yWf6OVXZFFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Eqf9D4nRZHAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnAH5mJhZJeJ",
        "outputId": "ac28aa42-3c4f-4ee1-977d-2df6d83d30fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 84) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jwLnwSfZOE0",
        "outputId": "ebf38445-8432-4b7f-e3fe-f75ea58239a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           21504     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 84)            86100     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,532,692\n",
            "Trainable params: 30,532,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RUOx3KtZbpC",
        "outputId": "530ff5d3-9467-4cce-cb3a-0876b2d9c422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.430464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "1orK5BU2aEoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./training_checkpoints"
      ],
      "metadata": {
        "id": "o0E75zNiZjKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./training_checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRLLRdVKZkui",
        "outputId": "c7797486-f46b-4b2d-c07c-dd319952756b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './training_checkpoints': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_freq=88*5,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "OCHbQP0OZmHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60"
      ],
      "metadata": {
        "id": "EinLPGn_Z-FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CRQcxuXZ-_j",
        "outputId": "442fc101-a65b-4762-dc05-7b9bbf2c5632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 1.1693\n",
            "Epoch 2/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 1.1196\n",
            "Epoch 3/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 1.0788\n",
            "Epoch 4/60\n",
            "266/266 [==============================] - 101s 372ms/step - loss: 1.0457\n",
            "Epoch 5/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 1.0152\n",
            "Epoch 6/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.9885\n",
            "Epoch 7/60\n",
            "266/266 [==============================] - 100s 371ms/step - loss: 0.9640\n",
            "Epoch 8/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.9393\n",
            "Epoch 9/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.9162\n",
            "Epoch 10/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.8930\n",
            "Epoch 11/60\n",
            "266/266 [==============================] - 99s 367ms/step - loss: 0.8723\n",
            "Epoch 12/60\n",
            "266/266 [==============================] - 100s 371ms/step - loss: 0.8505\n",
            "Epoch 13/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.8298\n",
            "Epoch 14/60\n",
            "266/266 [==============================] - 100s 372ms/step - loss: 0.8086\n",
            "Epoch 15/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.7881\n",
            "Epoch 16/60\n",
            "266/266 [==============================] - 99s 367ms/step - loss: 0.7667\n",
            "Epoch 17/60\n",
            "266/266 [==============================] - 100s 372ms/step - loss: 0.7455\n",
            "Epoch 18/60\n",
            "266/266 [==============================] - 99s 367ms/step - loss: 0.7243\n",
            "Epoch 19/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.7033\n",
            "Epoch 20/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.6825\n",
            "Epoch 21/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.6597\n",
            "Epoch 22/60\n",
            "266/266 [==============================] - 100s 369ms/step - loss: 0.6399\n",
            "Epoch 23/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.6188\n",
            "Epoch 24/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.5969\n",
            "Epoch 25/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.5770\n",
            "Epoch 26/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.5588\n",
            "Epoch 27/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.5383\n",
            "Epoch 28/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.5192\n",
            "Epoch 29/60\n",
            "266/266 [==============================] - 100s 371ms/step - loss: 0.5007\n",
            "Epoch 30/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.4839\n",
            "Epoch 31/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.4665\n",
            "Epoch 32/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.4495\n",
            "Epoch 33/60\n",
            "266/266 [==============================] - 98s 366ms/step - loss: 0.4361\n",
            "Epoch 34/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.4206\n",
            "Epoch 35/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.4070\n",
            "Epoch 36/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.3952\n",
            "Epoch 37/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3843\n",
            "Epoch 38/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.3730\n",
            "Epoch 39/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3638\n",
            "Epoch 40/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3537\n",
            "Epoch 41/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.3454\n",
            "Epoch 42/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3378\n",
            "Epoch 43/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3312\n",
            "Epoch 44/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.3243\n",
            "Epoch 45/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.3169\n",
            "Epoch 46/60\n",
            "266/266 [==============================] - 98s 364ms/step - loss: 0.3118\n",
            "Epoch 47/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.3076\n",
            "Epoch 48/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.3032\n",
            "Epoch 49/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.2991\n",
            "Epoch 50/60\n",
            "266/266 [==============================] - 100s 371ms/step - loss: 0.2958\n",
            "Epoch 51/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.2911\n",
            "Epoch 52/60\n",
            "266/266 [==============================] - 99s 368ms/step - loss: 0.2868\n",
            "Epoch 53/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.2845\n",
            "Epoch 54/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.2805\n",
            "Epoch 55/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.2787\n",
            "Epoch 56/60\n",
            "266/266 [==============================] - 99s 366ms/step - loss: 0.2767\n",
            "Epoch 57/60\n",
            "266/266 [==============================] - 99s 369ms/step - loss: 0.2736\n",
            "Epoch 58/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.2730\n",
            "Epoch 59/60\n",
            "266/266 [==============================] - 98s 365ms/step - loss: 0.2707\n",
            "Epoch 60/60\n",
            "266/266 [==============================] - 100s 370ms/step - loss: 0.2676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SriAh6et9npK",
        "outputId": "b44bbbe6-3b7a-4116-d3b2-ef981c5f9409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_60'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "SLApLEHo9toQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwpclm_Z9vRe",
        "outputId": "762f1d30-f89a-48e3-99a6-a7c939a38268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (1, None, 256)            21504     \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (1, None, 1024)           8392704   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (1, None, 84)             86100     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,532,692\n",
            "Trainable params: 30,532,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "Ei1Ga9sN9x1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 0.1\n",
        "text_ = generate_text(model, start_string=u\"Biden win\")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URVWWgfH95WC",
        "outputId": "44c1ff1e-882a-44bd-f302-61bce073f4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biden wins. When we win, Pennsylvania wins and America wins. For decades our leaders put global interests and senators. I mean, would have been nice much easier, because here in Des Moines, Iowa. We stand on the shoulders of American heroes who crossed the ocean, blazed the trail, sailed the continent, tamed the wilderness, dug out the Panama canal, laid down the railroads, revolutionized industry, won two World Wars, defeated fascism and communism, and put a man on the face of the earth. And we are maki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ответ получился так себе, после введеных слов сеть решила поставить точку и начать новое предложение на другую тему, попробуем изменить температуру на 1"
      ],
      "metadata": {
        "id": "EpYhQIw2aD6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "text_ = generate_text(model, start_string=u\"Biden wins\")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXJpJfL69-09",
        "outputId": "7115afcd-807e-447b-82cb-de9cdef8d375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biden wins. I guess they had no choice and they have money, they go to end it. If I did it, and we worked hard, and it was like this, every place we go. Last night, you know where we were. You have an obligation to do it. So now they're all saying we win, I'll have to protect you.\" So they say, \"Yeah,\" but he couldn't draw a crowd. So they put a circle, like he's so sad the wall are my great, he owns We Try, right? Not a mistake when he heart and somebody said, of our farmers can do it. I'm not doing it. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот здесь получилось уже намного лучше, сеть логическую идею проталкивать от и почти до конца спича, если сильно вчитаться то видно логические ошибки, но в целом получилось хорошо"
      ],
      "metadata": {
        "id": "JPqwp2yLadKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем давать разные тексты связанные с политкий, экономикой или соц вопросами, и посмотрим на результат"
      ],
      "metadata": {
        "id": "mKinYMt2a0ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = generate_text(model, start_string=u\"Federal investigation of Hunter Biden reaches critical\")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "befBvE4jBjbs",
        "outputId": "a7f3cfb2-6ff4-4890-cdd7-637492151044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federal investigation of Hunter Biden reaches critical reasons because they don't want to say it. The radical left in the world and today, we will never ever … I promise to you, ride a breakup betrayears. That'll be very well. \"I'm down number one terrorist, shirted neithen that trey put with our great congressmen, they're here, frankly. So there's other four score anyour family and deciding the fate of your country, because he actually had a historic numbers left gets how are we doing constantly sort of a hey did anything. Let's Democrat Phatharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = generate_text(model, start_string=u\"America is a nation that can be defined in a single word\")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egqj0wunB_rs",
        "outputId": "47b8fb25-e015-4884-e955-05e137b3f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "America is a nation that can be defined in a single word, not one of them. They get it. And what about this Kamala? How about his polls done? Remember when China gives her their farm inbertion. Protestors, judges and justices who interpret the constitution as written is because they didn't want to do it because it sounds luxury. But at some point you got to buy a cureful with the people that control Biden's plan, in just three and a half years under my administration, we've secured America's borders, fixed our broke and really he deserves another six\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = generate_text(model, start_string=u\"Biden is willing to destroy America in order to destroy Russia\")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJu4EtWsEMjA",
        "outputId": "fb1edff8-f11d-4998-a91b-4a5a7cf14faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biden is willing to destroy America in order to destroy Russia, Russia, Russia. That's the eace brother, what? We did the opposite. I have to say of a tremendous man. We will surprise, think of that. We protect our border, and that's what we're doing, and that's what we're doing, but we're rising rape contracted by these countries. With Japan, we've got negotiating with the newction laws here in America, we've done a fantastic job watching a new crime, right? And it was too late before the election. They were trying to be nice, for other r. I wonder why? A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "text_ = generate_text(model, start_string=u\"Black Lives Matter abbreviated BLM is a decentralized political and social movement that seeks to highlight racism, discrimination, and racial inequality experienced by black people \")\n",
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzlTo-snH8Rk",
        "outputId": "d5e0b72a-495e-4961-8828-344eca3df333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Black Lives Matter abbreviated BLM is a decentralized political and social movement that seeks to highlight racism, discrimination, and racial inequality experienced by black people that would destroyed Medicare as we keep on winning, Washington Democrats put the needs of illegal immigrants by the way. For that totall decide whether we will quickly return to record prosperity or whether we will always protect pre-existing conditions. America will land the first woman on the moon and the United States will be totally sick, don't worry about it. Who would even think this happened- And how bad drug overwhelm with a lot of help from around the world at risk. And the only reason\n"
          ]
        }
      ]
    }
  ]
}